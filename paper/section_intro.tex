\section{Introduction}
Since 1997, KDD Cup has been one of the most prestigious competitions in knowledge discovery and data mining, where experts around the world from both industry and academia compete with each other with best modeling practices to solve real world challenges in complex data sets.

Massive Open Online Course (MOOC) platforms aim at providing the mass population with open access to quality education.  Although their initial success in a few courses, MOOC platforms have struggled with extremely high dropout rates. Perna et al. reported that the average completion rate is 4\% among 1 million students across 16 Coursera courses offered by the University of Pennsylvania from June 2012 to June 2013 \cite{perna2013life}.  If we identify those who are likely to drop out, we can engage with and help them complete courses successfully.

Therefore, KDD Cup 2015 asks participants to predict the likelihood of dropout for students based on their activity logs, course enrollment, and course material data provided by XuetangX, one of the largest MOOC platforms in China.
Activity logs of 200,906 enrollments from 112,448 students across 39 courses are provided.
Each activity is described by 6 fields of the username, course ID, timestamp, source, event, and object. 
For each object, 3 additional fields of the category, children, and start date are provided.
The training set consists of 8,157,278 logs from 120,543 enrollments with the target variable indicating if a student dropped out.  
The test set consists of 5,387,848 logs from 80,363 enrollments.
The full description of the data sets is available in \cite{kddcup2015_data}.

Our final solution is a joint work from 9 data scientists, distributed around the world.
The pipeline from raw data to final solution is as follows:
\begin{itemize}
  \setlength\itemsep{0em}
  \item Hand crafted feature engineering (most of hard work)
  \item Automatic feature design (autoencoder)
  \item Individual models (gbm, nn, factor model,..)
  \item Stage-I ensemble (blends individual models)
  \item Stage-II ensemble (blends stage-I ensemble models)
  \item Stage-III ensemble (blends stage-II ensemble models)
\end{itemize}

The rest of the paper is organized as follows. Section 2 describes our feature engineering approach. Section 3 introduces various classification algorithms used to train single classifiers. Section 4 overviews our multi-stage ensemble framework. Section 5 shows our final solution. Section 6 concludes the paper.

% I think our ensemble framework is different from Otto competition winner's:  Theirs is a stage-II and ours is a stage-III.
% This approach was published by a winning team of Otto kaggle competition \cite{otto},\cite{triskelion}.