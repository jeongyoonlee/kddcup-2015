\section{Single Models}
something like this.

\subsection{Learning Algorithms}
\begin{itemize}
\setlength\itemsep{0em}
\item Logistic Regression (LR)
\item Kernel Ridge Regression (KRR)
\item Factorization Machine/Field-aware Factorization Machine (FM/FFM)
\item Neural Networks (NN)
\item Extreme Trees (ET)
\item Gradient Boosting Decision Trees (GBDT)
\end{itemize}

\subsection{Single Models}
\begin{itemize}
  \setlength\itemsep{0em}
  \item Model 1: RandomForest(R). Dataset: X
  \item Model 2: Logistic Regression(scikit). Dataset: Log(X+1)
  \item Model 3: Extra Trees Classifier(scikit). Dataset: Log(X+1) (but could be raw)
  \item Model 4: KNeighborsClassifier(scikit). Dataset: Scale( Log(X+1) )
  \item Model 5: libfm. Dataset: Sparse(X). Each feature value is a unique level.
\end{itemize}

\begin{table*}[t]
\begin{center}
\begin{tabular}{lllll}
\label{tb:singleModels}
ID	& Model 				& Feature										& 5-CV		& Public Leaderboard \\ \hline
S1 	& xgb\_rf\_ko\_new\_feat 	& - 											& 0.906721	& 0.907765 \\
S2 	& ko\_v83			 	& Feature RW + Feature SK + Feature JL + ko\_feat3 	& 0.906729	& 0.907525\\
S3 	& bag\_10\_xgb\_rf.xiv  	& Feature RW + Feature SK + Feature TN 			& 0.905875 	& 0.906361 \\
S4 	& xgb\_rf.xvi			& Feature RW + Feature TN + Feature JL				& 0.905543	& 0.905516 \\
S5 	& xgb\_rf.xix			& Feature RW + Feature SK + Feature TN + Feature MB	& 0.905356	& - \\
S6	& xgb\_rf.xv			& Feature RW + Feature TN						& 0.904935	& 0.905480 \\
S7	& xgb\_rw\_sk			& Feature RW + Feature SK						& 0.904914	& - \\
S8	& mjahrer.feat.tam+rw+sk+tam2+sk2+azure.dae+nn	& Feature RW + Feature SK + Feature TN + Feature MJ & 0.904235 & - \\
S9	& mjahrer.featmjahrer+Tam+RW+KoheiSong.nn & Feature RW + Feature SK + Feature TN + Feature MJ & 0.903736 & - \\
S10	& mjahrer.featTam+RW+KoheiSong.dae+nn & Feature RW + Feature SK + Feature TN & 0.903669	& - \\
S11	& pred\_train\_sk\_feature\_ffm\_rw & - 									& 0.903560	& 0.904411 \\
S12	& mjahrer.featTam+RW+KoheiSong.nn & Feature RW + Feature SK + Feature TN 	& 0.903428	& - \\
S13	& xg\_400\_4\_0.05\_feature\_rw & Feature RW + Feature JL					& 0.903385	& - \\
S14 	& kohei\_song			& Feature SK									& 0.902918	& - \\
S15 	& xgb\_cv\_rw			& Feature RW 									& 0.902287	& - \\
S16	& ffm\_cv\_rw			& Feature RW									& 0.901983	& - \\
S17	& mjahrer.dae+nn.RWfeat	& Feature RW									& 0.901846	& 0.902614 \\
S18	& mjahrer.featmjahrer+Tam+RW+KoheiSong.dae+krr & Feature RW + Feature SK + Feature TN + Feature MJ	& 0.901522	& - \\
S19	& xg\_400\_4\_0.05\_feature\_sk	& Feature SK							& 0.900906	& - \\
S20	& xgb\_rf.xiv			& Feature TN									& 0.899239	& - \\
S21	& xgb\_rf.xiv			& Feature TN									& 0.899167	& - \\
S22	& xgb\_rf.xiv			& Feature TN									& 0.898969	& - \\
S23 	& xgb\_rf.xiv			& Feature TN									& 0.898890	& - \\
S24 	& xgb.xiv				& Feature TN									& 0.898749	& - \\
S25	& libfm\_100\_4\_0.002\_feature\_rw\_v2	& Feature RW + Feature JL			& 0.898308	& - \\
S26  & xgl\_500\_0.5\_10\_10\_feature\_rw\_v2	& Feature RW + Feature JL			& 0.897968	& - \\
S27	& xgb\_val.xiii			& Feature TN									& 0.897912	& - \\
S28	& nn\_20\_16\_0.01\_feature\_rw\_v2	& Feature RW + Feature JL			& 0.897143	& - \\
S29	& mb\_nn\_50\_20\_feat19	& Feature RW + Feature MB					& 0.896748	& - \\
S30  & ffm\_30\_20\_0.01\_feature\_rw\_v2	& Feature RW + Feature JL			& 0.896160	& - \\
S31  & xg\_400\_4\_0.05\_feature\_tam	& Feature TN							& 0.895754	& - \\
S32	& ConfigAMLKRR		& Feature MJ									& 0.894524	& - \\
S33 	& gbm.xiii				& Feature TN									& 0.893507	& - \\
S34	& xg\_600\_4\_0.05\_feature10	& Feature JL								& 0.892364	& - \\
S35 	& xg\_600\_4\_0.05\_feature9	& Feature JL								& 0.892253	& - \\
S36 	& ConfigAML			& Feature MJ									& 0.891217	& - \\
S37	& lr\_0.01\_feature\_tam	& Feature TN									& 0.890580	& - \\
S38	& ConfigAMLCUDAPreModel	& Feature MJ								& 0.890565	& - \\
S39 	& ffm\_30\_20\_0.01\_feature\_tam	& Feature TN							& 0.888418	& - \\
S40	& libfm\_100\_4\_0.002\_feature\_tam	& Feature TN						& 0.888381	& - \\
S41	& rf.xiii				& Feature TN									& 0.887583	& - \\
S42	& et\_1000\_20\_feature\_tam	& Feature TN								& 0.887768	& - \\
S43 	& ffm\_30\_20\_0.01\_feature9	& Feature JL								& 0.887116	& - \\
S44	& libfm\_100\_4\_0.002\_feature9	& Feature JL							& 0.886866	& - \\
S45	& ConfigAML			& Feature MJ									& 0.886705	& - \\
S46	& nn\_20\_16\_0.01\_feature9	& Feature JL								& 0.886109	& - \\
S47 	& xg\_400\_4\_0.05\_feature6	& Feature JL								& 0.885184	& - \\
S48 	& xg\_400\_4\_0.05\_feature3	& Feature JL								& 0.885124	& - \\
S49 	& ffm\_20\_20\_0.01\_feature6	& Feature JL								& 0.885037	& - \\
S50	& ffm\_20\_20\_0.01\_feature3	& Feature JL								& 0.884697	& - \\
S51 	& xg\_400\_4\_0.05\_feature\_mj	& Feature MJ							& 0.882441	& - \\
S52	& et\_1000\_20\_feature\_rw\_v2	& Feature RW + Feature JL				& 0.881539	& - \\
S53	& libfm\_100\_8\_0.01\_feature6	& Feature JL							& 0.880878	& - \\
S54	& libfm\_100\_8\_0.01\_feature3	& Feature JL							& 0.880366	& - \\
S55	& nn\_20\_16\_0.005\_feature3	& Feature JL								& 0.880219	& - \\
S56	& ConfigAMLKNN		& Feature MJ									& 0.877652	& - \\
S57	& nn\_20\_16\_0.005\_feature5	& Feature JL								& 0.876905	& - \\
S58	& lr\_0.01\_feature\_mj		& Feature MJ								& 0.821332	& - \\
S59	& lr\_0.01\_feature9		& Feature JL									& 0.804150	& - \\
S60	& lr\_0.01\_feature3		& Feature JL									& 0.802138	& - \\
S61	& lr\_0.01\_feature6		& Feature JL									& 0.800225	& - \\
\end{tabular}
\end{center}
\end{table*}